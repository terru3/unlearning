{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5202ad22-3c0d-4105-a252-bdc13e42faae",
   "metadata": {},
   "source": [
    "# Amnesiac Unlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064da061-2af6-4507-b5ac-702a13e10cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: CNN_CIFAR_100_ORIGINAL_23\n"
     ]
    }
   ],
   "source": [
    "path = \"./\"\n",
    "root = \"../\"\n",
    "\n",
    "SEED = 23\n",
    "BATCH_SIZE = 128\n",
    "LR = 1e-3\n",
    "PRINT_ITERS = 50\n",
    "\n",
    "MODEL_NAME = (\n",
    "    f\"CNN_CIFAR_100_ORIGINAL_{SEED}\"\n",
    ")\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4cf368c-b887-46de-8afe-f6273fae1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf0e47a5-ef43-4648-a0be-b3c321766a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = None\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15037760-5e85-4e1e-a228-ebb01f954baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "path = path if drive is None else \"/content/drive/MyDrive/unlearning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba00e9c8-ea5b-458d-8067-2a4dfb75cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed) # if multi-GPU\n",
    "    torch.backends.cudnn.deterministic=True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80708e66-0e2f-4db2-82ec-b46d802f0ca3",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ab40ecb-ad6b-449a-9301-0b0c2b84962f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform_fn = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        # precomputed CIFAR100 mean and std\n",
    "        transforms.Normalize(mean=(0.5071, 0.4865, 0.4409), std=(0.2673, 0.2564, 0.2762))\n",
    "    ])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.CIFAR100(root='./data', train=True, download=True,\n",
    "                                transform=transform_fn),\n",
    "                                batch_size=BATCH_SIZE, shuffle=False)\n",
    "# no shuffle to keep order of batches consistent for our use case\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.CIFAR100(root='./data', train=False, download=True,\n",
    "                                transform=transform_fn),\n",
    "                                batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "fine_labels = [\n",
    "    'apple',\n",
    "    'aquarium_fish',\n",
    "    'baby',\n",
    "    'bear',\n",
    "    'beaver',\n",
    "    'bed',\n",
    "    'bee',\n",
    "    'beetle',\n",
    "    'bicycle',\n",
    "    'bottle',\n",
    "    'bowl',\n",
    "    'boy',\n",
    "    'bridge',\n",
    "    'bus',\n",
    "    'butterfly',\n",
    "    'camel',\n",
    "    'can',\n",
    "    'castle',\n",
    "    'caterpillar',\n",
    "    'cattle',\n",
    "    'chair',\n",
    "    'chimpanzee',\n",
    "    'clock',\n",
    "    'cloud',\n",
    "    'cockroach',\n",
    "    'couch',\n",
    "    'crab',\n",
    "    'crocodile',\n",
    "    'cup',\n",
    "    'dinosaur',\n",
    "    'dolphin',\n",
    "    'elephant',\n",
    "    'flatfish',\n",
    "    'forest',\n",
    "    'fox',\n",
    "    'girl',\n",
    "    'hamster',\n",
    "    'house',\n",
    "    'kangaroo',\n",
    "    'computer_keyboard',\n",
    "    'lamp',\n",
    "    'lawn_mower',\n",
    "    'leopard',\n",
    "    'lion',\n",
    "    'lizard',\n",
    "    'lobster',\n",
    "    'man',\n",
    "    'maple_tree',\n",
    "    'motorcycle',\n",
    "    'mountain',\n",
    "    'mouse',\n",
    "    'mushroom',\n",
    "    'oak_tree',\n",
    "    'orange',\n",
    "    'orchid',\n",
    "    'otter',\n",
    "    'palm_tree',\n",
    "    'pear',\n",
    "    'pickup_truck',\n",
    "    'pine_tree',\n",
    "    'plain',\n",
    "    'plate',\n",
    "    'poppy',\n",
    "    'porcupine',\n",
    "    'possum',\n",
    "    'rabbit',\n",
    "    'raccoon',\n",
    "    'ray',\n",
    "    'road',\n",
    "    'rocket',\n",
    "    'rose',\n",
    "    'sea',\n",
    "    'seal',\n",
    "    'shark',\n",
    "    'shrew',\n",
    "    'skunk',\n",
    "    'skyscraper',\n",
    "    'snail',\n",
    "    'snake',\n",
    "    'spider',\n",
    "    'squirrel',\n",
    "    'streetcar',\n",
    "    'sunflower',\n",
    "    'sweet_pepper',\n",
    "    'table',\n",
    "    'tank',\n",
    "    'telephone',\n",
    "    'television',\n",
    "    'tiger',\n",
    "    'tractor',\n",
    "    'train',\n",
    "    'trout',\n",
    "    'tulip',\n",
    "    'turtle',\n",
    "    'wardrobe',\n",
    "    'whale',\n",
    "    'willow_tree',\n",
    "    'wolf',\n",
    "    'woman',\n",
    "    'worm',\n",
    "]\n",
    "\n",
    "# not using for now. only doing fine classification\n",
    "mapping_coarse_fine = {\n",
    "    'aquatic mammals': ['beaver', 'dolphin', 'otter', 'seal', 'whale'],\n",
    "    'fish': ['aquarium_fish', 'flatfish', 'ray', 'shark', 'trout'],\n",
    "    'flowers': ['orchid', 'poppy', 'rose', 'sunflower', 'tulip'],\n",
    "    'food containers': ['bottle', 'bowl', 'can', 'cup', 'plate'],\n",
    "    'fruit and vegetables': ['apple', 'mushroom', 'orange', 'pear',\n",
    "                             'sweet_pepper'],\n",
    "    'household electrical device': ['clock', 'computer_keyboard', 'lamp',\n",
    "                                    'telephone', 'television'],\n",
    "    'household furniture': ['bed', 'chair', 'couch', 'table', 'wardrobe'],\n",
    "    'insects': ['bee', 'beetle', 'butterfly', 'caterpillar', 'cockroach'],\n",
    "    'large carnivores': ['bear', 'leopard', 'lion', 'tiger', 'wolf'],\n",
    "    'large man-made outdoor things': ['bridge', 'castle', 'house', 'road',\n",
    "                                      'skyscraper'],\n",
    "    'large natural outdoor scenes': ['cloud', 'forest', 'mountain', 'plain',\n",
    "                                     'sea'],\n",
    "    'large omnivores and herbivores': ['camel', 'cattle', 'chimpanzee',\n",
    "                                       'elephant', 'kangaroo'],\n",
    "    'medium-sized mammals': ['fox', 'porcupine', 'possum', 'raccoon', 'skunk'],\n",
    "    'non-insect invertebrates': ['crab', 'lobster', 'snail', 'spider', 'worm'],\n",
    "    'people': ['baby', 'boy', 'girl', 'man', 'woman'],\n",
    "    'reptiles': ['crocodile', 'dinosaur', 'lizard', 'snake', 'turtle'],\n",
    "    'small mammals': ['hamster', 'mouse', 'rabbit', 'shrew', 'squirrel'],\n",
    "    'trees': ['maple_tree', 'oak_tree', 'palm_tree', 'pine_tree',\n",
    "              'willow_tree'],\n",
    "    'vehicles 1': ['bicycle', 'bus', 'motorcycle', 'pickup_truck', 'train'],\n",
    "    'vehicles 2': ['lawn_mower', 'rocket', 'streetcar', 'tank', 'tractor'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7920c1c4-6133-4ae8-b3c5-7432b769060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # (128, 3, 32, 32), picture of a mountain\n",
    "\n",
    "# batch = next(iter(train_loader))\n",
    "# print(batch[0].shape)\n",
    "# test_idx = 42\n",
    "# plt.imshow(batch[0][test_idx].permute(1,2,0))\n",
    "# plt.title(f'{fine_labels[batch[1][test_idx]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c5a772b-b780-4059-98f5-84687463d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, 1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, 1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, 1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.batchnorm2d_1 = nn.BatchNorm2d(128)\n",
    "        self.batchnorm2d_2 = nn.BatchNorm2d(512)\n",
    "        self.batchnorm1d = nn.BatchNorm1d(128) # after fc1\n",
    "        self.fc1 = nn.Linear(512*2*2, 128)\n",
    "        self.fc2 = nn.Linear(128, 100) # 100 classes for fine labels\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batchnorm2d_1(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.batchnorm2d_2(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.batchnorm1d(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563de37-02ca-4cdb-beba-af0cbf6c095f",
   "metadata": {},
   "source": [
    "# Amnesiac Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3af63a07-0d44-4dda-b539-42405c5a1219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cloud'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_class = 23\n",
    "fine_labels[target_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da45ed94-79f3-4a2f-a703-688ad288c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amnesiac_train(model, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Trains the specified model, returning train losses, validation losses, validation accuracies, and\n",
    "    parameter updates for batches containing data of the sensitive class. Currently does not support\n",
    "    multiple sensitive classes.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_losses, val_losses = [], []\n",
    "    val_accuracies = []\n",
    "    deltas = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        delta = {}\n",
    "        for name, _ in model.named_parameters(): \n",
    "            if 'weight' in name or 'bias' in name:\n",
    "                delta[name] = 0\n",
    "\n",
    "        for step, (img, label) in enumerate(train_loader):\n",
    "\n",
    "            if target_class in label:\n",
    "                pre = {}\n",
    "                for name, param in model.named_parameters(): \n",
    "                    if 'weight' in name or 'bias' in name:\n",
    "                        pre[name] = param.data.clone()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            train_losses.append(loss.item()) # every step\n",
    "            loss.backward()\n",
    "    \n",
    "            # Monitoring overall gradient norm\n",
    "            grads = [\n",
    "                    param.grad.detach().flatten()\n",
    "                    for param in model.parameters()\n",
    "                    if param.grad is not None\n",
    "                ]\n",
    "            norm = torch.cat(grads).norm()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "            if target_class in label:\n",
    "                post = {}\n",
    "                for name, param in model.named_parameters(): \n",
    "                    if 'weight' in name or 'bias' in name:\n",
    "                        post[name] = param.data.clone()\n",
    "                for key in pre:\n",
    "                    delta[key] = delta[key] + (post[key] - pre[key])\n",
    "                        \n",
    "            if step % PRINT_ITERS == 0 and step != 0:\n",
    "                val_loss, val_acc = eval(epoch, step)\n",
    "                val_losses.append(val_loss)\n",
    "                val_accuracies.append(val_acc)\n",
    "                print(f\"Step: {step}/{len(train_loader)}, Running Average Loss: {np.mean(train_losses):.3f} |\",\n",
    "                      f\"Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.3f} | Grad Norm: {norm:.2f}\")\n",
    "                \n",
    "        deltas.append(delta)\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            },\n",
    "            f\"{path}/checkpoints/{MODEL_NAME}_STEP_{step}_SEED_{SEED}.pt\",\n",
    "        )\n",
    "    \n",
    "        with open(\n",
    "            f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_train_losses.json\", \"w\"\n",
    "        ) as f:\n",
    "            json.dump(train_losses, f)\n",
    "    \n",
    "        with open(\n",
    "            f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_val_losses.json\", \"w\"\n",
    "        ) as f2:\n",
    "            json.dump(val_losses, f2)\n",
    "    \n",
    "        with open(\n",
    "            f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_val_accuracies.json\", \"w\"\n",
    "        ) as f3:\n",
    "            json.dump(val_aux_losses, f3)\n",
    "\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies, deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7c26eef-743f-48a1-ba71-f11e04cd4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(epoch, i):\n",
    "    val_losses = []\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (img, label) in enumerate(val_loader):\n",
    "            out = model(img)\n",
    "            \n",
    "            loss_eval = criterion(out, label)\n",
    "            val_losses.append(loss_eval.item())\n",
    "            \n",
    "            pred = out.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_acc = correct / (len(val_loader) * BATCH_SIZE)\n",
    "    \n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e076c-a6b4-42b3-9401-b36769cc2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "model = Net()\n",
    "# summary(model)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05f23d-2ac3-404a-b8e3-e8a0606c5669",
   "metadata": {},
   "source": [
    "# Driver code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef3f2a-d8be-4234-adfa-9ac345904cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses, val_accuracies = amnesiac_train(model, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c277567f-3205-438b-b736-d468d68df00e",
   "metadata": {},
   "source": [
    "# Unlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f5ad2bf0-fd76-48aa-9cdd-7cebb47a2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unlearn(model, deltas):\n",
    "    for delta in deltas:\n",
    "        with torch.no_grad():\n",
    "            state = model.state_dict()\n",
    "            for name, param in model.named_parameters(): \n",
    "                if 'weight' in name or 'bias' in name:\n",
    "                    state[name] = state[name] - delta[name]\n",
    "            model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d6eef274-fdbd-4def-a6e3-7329a59e5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearn(model, deltas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
