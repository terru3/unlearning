{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4b5f20-1af9-4c76-9817-13a502b84e84",
   "metadata": {},
   "source": [
    "# Unlearning by Selective Impair and Repair (UNSIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb95043-53a1-4690-aaee-69299f3f93cf",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2111.08947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8b9b82-cd89-4401-aa1d-a2ee01675e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209e2728-6ead-40f7-96f0-1f2ef7646a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = None\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7be4ef1-65d0-4d0a-8b3c-e54154f2f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a5a33d-7779-458d-a0bd-678e15f207e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "path = path if drive is None else \"/content/drive/MyDrive/self-learn/unlearning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbbd3f0a-951f-442b-a838-82d0177bb616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from constants import *\n",
    "from utils import set_seed, train_data, val_data, \\\n",
    "                    train_loader, val_loader, fine_labels\n",
    "from models import get_model_and_optimizer\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f295fa22-3134-4c48-a717-106da164bb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: CNN_CIFAR_100_ORIGINAL\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = (\n",
    "    f\"CNN_CIFAR_100_ORIGINAL\"\n",
    ")\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbaed7e-f154-4f09-9dc7-23d984921c83",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764328cc-4458-4879-9c19-88caad82698a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cloud'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_class = 23\n",
    "fine_labels[target_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e85115-1540-438a-b44d-f02d47a979a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, val_loader, criterion, device):\n",
    "    val_losses = []\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (img, label) in enumerate(val_loader):\n",
    "          \n",
    "            img, label = img.to(device), label.to(device)\n",
    "            out = model(img)\n",
    "            \n",
    "            loss_eval = criterion(out, label)\n",
    "            val_losses.append(loss_eval.item())\n",
    "            \n",
    "            pred = out.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_acc = correct / (len(val_loader) * BATCH_SIZE)\n",
    "    \n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a105a27-a151-42ca-83b7-4b681167763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_idx = np.where(np.array(train_data.targets) == target_class)[0]\n",
    "forget_mask = np.zeros(len(train_data.targets), dtype=bool)\n",
    "forget_mask[forget_idx] = True\n",
    "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
    "\n",
    "forget_data = torch.utils.data.Subset(train_data, forget_idx)\n",
    "retain_data = torch.utils.data.Subset(train_data, retain_idx)\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(forget_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "retain_loader = torch.utils.data.DataLoader(retain_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c1181bd9-139e-4c31-8fc1-2abf3cab0162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and optimizer loaded\n"
     ]
    }
   ],
   "source": [
    "LOAD_EPOCH = 100\n",
    "\n",
    "model, optimizer = get_model_and_optimizer()\n",
    "model.load_state_dict(torch.load(f\"{path}/checkpoints/{MODEL_NAME}_EPOCH_{LOAD_EPOCH}_SEED_{SEED}.pt\",\n",
    "                                  map_location=device)[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(torch.load(f\"{path}/checkpoints/{MODEL_NAME}_EPOCH_{LOAD_EPOCH}_SEED_{SEED}.pt\",\n",
    "                                  map_location=device)[\"optimizer_state_dict\"])\n",
    "model.to(device)\n",
    "print('Model and optimizer loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f43aa316-ec64-4c07-9546-0164fb9c1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bf986-ce68-4ba8-a4a7-ee32546d2cc3",
   "metadata": {},
   "source": [
    "# TODO: UNSIR Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c20946-9a3b-4266-bb3e-50dbe20bb344",
   "metadata": {},
   "source": [
    "## TODO: Validate code below, run and try\n",
    "#### also TODO: add eval / running batch accuracies in impair and repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5a2c067b-7686-483a-a231-309b77293dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(nn.Module):\n",
    "    def __init__(self, *dim):\n",
    "        super().__init__()\n",
    "        self.noise = torch.nn.Parameter(torch.randn(*dim), requires_grad = True)\n",
    "        \n",
    "    def forward(self):\n",
    "        return self.noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63c82ac-c939-403d-bb87-63f750f545ad",
   "metadata": {},
   "source": [
    "## Step 1: Train noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7ba14e76-27e4-4664-beb4-f02ad08e25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = {}\n",
    "set_seed()\n",
    "noises[target_class] = Noise(BATCH_SIZE, 3, 32, 32).to(device)\n",
    "noise_optimizer = torch.optim.AdamW(noises[target_class].parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "cf62a1f9-133a-4b02-9515-1e880a554009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_noise(model, noises, noise_optimizer, target_class, device):\n",
    "    for epoch in range(NOISE_EPOCHS):\n",
    "        noise_train_losses = []\n",
    "        for i in range(NOISE_STEPS):\n",
    "            noise_optimizer.zero_grad()\n",
    "            input = noises[target_class]()\n",
    "            label = torch.full((BATCH_SIZE,), target_class)\n",
    "            out = model(input)\n",
    "            loss = -criterion(out, label) + NOISE_LAMBDA*torch.mean(torch.sum(torch.square(input), [1, 2, 3]))\n",
    "            loss.backward()\n",
    "            noise_train_losses.append(loss.item())\n",
    "            noise_optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch: {epoch+1}/{NOISE_EPOCHS}, Loss: {np.mean(noise_train_losses):.3f}\")\n",
    "        # torch.save(\n",
    "        #     {\n",
    "        #         \"model_state_dict\": model.state_dict(),\n",
    "        #         \"optimizer_state_dict\": noise_optimizer.state_dict(),\n",
    "        #     },\n",
    "        #     f\"{path}/checkpoints/NOISE_EPOCH_{epoch+1}_SEED_{SEED}.pt\",\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "319a8e39-cad8-4191-b72b-928f5f6b5c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5, Loss: 168.424\n",
      "Epoch: 2/5, Loss: 27.782\n",
      "Epoch: 3/5, Loss: -8.292\n",
      "Epoch: 4/5, Loss: -13.100\n",
      "Epoch: 5/5, Loss: -16.482\n"
     ]
    }
   ],
   "source": [
    "train_noise(model, noises, noise_optimizer, target_class, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c9655-635a-437b-8b25-25aab953c1cc",
   "metadata": {},
   "source": [
    "## Step 2: Impair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6c117ec0-797b-4fbe-a85b-e5cf6d3a06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep noisy data loader, combine with retain data\n",
    "noise_data = []\n",
    "\n",
    "for i in range(NUM_NOISE_BATCHES):\n",
    "    batch = noises[target_class]().cpu().detach()\n",
    "    for i in range(BATCH_SIZE):\n",
    "        noise_data.append((batch[i], target_class))\n",
    "\n",
    "## TO-CONSIDER: Instead of adding all of retain_data, add only 10% of it\n",
    "# subset_idx = list(range(0, len(retain_data), 10))\n",
    "# retain_data_subset = torch.utils.data.Subset(retain_data, subset_idx)\n",
    "# noise_data += retain_data_subset\n",
    "\n",
    "noise_data += retain_data # takes 15 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "060f4703-92ab-4b88-9de0-419442972f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_loader = torch.utils.data.DataLoader(noise_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "impair_optimizer = torch.optim.AdamW(model.parameters(), lr=IMPAIR_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "faf9633e-e2b5-44fb-8035-1c90f46dc7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impair(model, impair_optimizer, noisy_loader, device):\n",
    "    model.train()\n",
    "    impair_train_losses = []\n",
    "    for epoch in range(IMPAIR_EPOCHS):\n",
    "        for step, (img, label) in enumerate(noisy_loader):\n",
    "            img, label = img.to(device), label.to(device)\n",
    "        \n",
    "            impair_optimizer.zero_grad()\n",
    "            out = model(input)\n",
    "            loss = criterion(out, label)\n",
    "            loss.backward()\n",
    "            impair_train_losses.append(loss.item())\n",
    "            impair_optimizer.step()\n",
    "\n",
    "            if step % 150 == 0 and step > 0:\n",
    "                print(f\"Step: {step}/{len(noisy_loader)}, Running Average Loss: {np.mean(impair_train_losses):.3f}\")\n",
    "            \n",
    "        # torch.save(\n",
    "        #     {\n",
    "        #         \"model_state_dict\": model.state_dict(),\n",
    "        #         \"optimizer_state_dict\": impair_optimizer.state_dict(),\n",
    "        #     },\n",
    "        #     f\"{path}/checkpoints/{MODEL_NAME}_IMPAIR_EPOCH_{epoch+1}_SEED_{SEED}.pt\",\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4e9ff38f-68e5-463a-8ee6-eba1263d6c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 20/6288, Running Average Loss: 6.484\n",
      "Step: 40/6288, Running Average Loss: 5.678\n",
      "Step: 60/6288, Running Average Loss: 5.409\n",
      "Step: 80/6288, Running Average Loss: 5.254\n",
      "Step: 100/6288, Running Average Loss: 5.152\n",
      "Step: 120/6288, Running Average Loss: 5.084\n",
      "Step: 140/6288, Running Average Loss: 5.021\n",
      "Step: 160/6288, Running Average Loss: 4.985\n",
      "Step: 180/6288, Running Average Loss: 4.955\n",
      "Step: 200/6288, Running Average Loss: 4.929\n",
      "Step: 220/6288, Running Average Loss: 4.905\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimpair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimpair_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoisy_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[175], line 10\u001b[0m, in \u001b[0;36mimpair\u001b[0;34m(model, impair_optimizer, noisy_loader, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m out \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, label)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m impair_train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     12\u001b[0m impair_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/unlearning/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/unlearning/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "impair(model, impair_optimizer, noisy_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c41ab-1c99-4249-b89e-8a7531156245",
   "metadata": {},
   "source": [
    "## Step 3: Repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc9db7-ae90-4c83-b56a-3c92345b61d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "repair_optimizer = torch.optim.AdamW(model.parameters(), lr=REPAIR_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53083879-93ff-4fe6-bf19-4a79f9f20ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair(model, repair_optimizer, retain_loader, device):\n",
    "    model.train()\n",
    "    repair_train_losses = []\n",
    "    for epoch in range(REPAIR_EPOCHS):\n",
    "        for step, (img, label) in enumerate(retain_loader):\n",
    "            img, label = img.to(device), label.to(device)\n",
    "        \n",
    "            repair_optimizer.zero_grad()\n",
    "            out = model(input)\n",
    "            loss = criterion(out, label)\n",
    "            loss.backward()\n",
    "            repair_train_losses.append(loss.item())\n",
    "            repair_optimizer.step()\n",
    "\n",
    "            if step % 150 == 0 and step > 0:\n",
    "                print(f\"Step: {step}/{len(retain_loader)}, Running Average Loss: {np.mean(repair_train_losses):.3f}\")\n",
    "            \n",
    "        # torch.save(\n",
    "        #     {\n",
    "        #         \"model_state_dict\": model.state_dict(),\n",
    "        #         \"optimizer_state_dict\": repair_optimizer.state_dict(),\n",
    "        #     },\n",
    "        #     f\"{path}/checkpoints/{MODEL_NAME}_REPAIR_EPOCH_{epoch+1}_SEED_{SEED}.pt\",\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ee8b7-3d6c-4572-9746-d0b5f108641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "repair(model, repair_optimizer, retain_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def29fd-a3cb-48d5-adf9-70d3d3069d7c",
   "metadata": {},
   "source": [
    "# Driver code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c500f2-98db-4407-8e55-e82782ae5788",
   "metadata": {},
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "51e852dd-9b41-4358-b2ad-fcf6307693ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use shuffle for more interesting results\n",
    "val_viz_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "forget_viz_loader = torch.utils.data.DataLoader(forget_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f6734-a675-4e3a-b174-600cb712b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # choose one batch from val and one batch from forget\n",
    "    for (val_img, val_label), (forget_img, forget_label) in zip(val_viz_loader, forget_viz_loader):\n",
    "        viz_img, viz_label = torch.cat([val_img, forget_img]), torch.cat([val_label, forget_label])\n",
    "        viz_img, viz_label = viz_img.to(device), viz_label.to(device)\n",
    "        out = model(viz_img)\n",
    "        pred = out.argmax(dim=-1)\n",
    "        break\n",
    "\n",
    "# assumes BATCH_SIZE=8\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16,12))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.set_title(f\"Pred: {fine_labels[pred[i]]} | Label: {fine_labels[viz_label[i]]}\", fontsize=8)\n",
    "    ax.imshow(invTrans(viz_img[i]).cpu().permute(1,2,0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
