{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5202ad22-3c0d-4105-a252-bdc13e42faae",
   "metadata": {},
   "source": [
    "# Unlearning on Vision Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102f4f0-6a5f-4c4e-903d-ef4daf5bbc9d",
   "metadata": {},
   "source": [
    "## TODO: write tiny ViT model in models, download and set up CIFAR-10, train on CIFAR-10 and test various unlearning methods such as SSD, on combinations of Q,K,V matrices, or perhaps the MLP, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d421a-a87e-456d-a0d3-0326559f9d57",
   "metadata": {},
   "source": [
    "# ––––––––––"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4cf368c-b887-46de-8afe-f6273fae1541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf0e47a5-ef43-4648-a0be-b3c321766a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = None\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab55a7a-929d-447d-a5d3-09f9d97b617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15037760-5e85-4e1e-a228-ebb01f954baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "path = path if drive is None else \"/content/drive/MyDrive/self-learn/unlearning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38255504-28ec-413d-be85-0bd44a066adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba00e9c8-ea5b-458d-8067-2a4dfb75cfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from constants import *\n",
    "from utils import set_seed\n",
    "from cifar-10-utils import train_data, val_data, train_loader, val_loader, invTrans\n",
    "from models import get_vit_and_optimizer, get_attack_model_and_optimizer\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d68cbaf-3678-4f56-ad09-15557f81aaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: CNN_CIFAR_100_ORIGINAL\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = f\"ViT_CIFAR_10_ORIGINAL\"\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80708e66-0e2f-4db2-82ec-b46d802f0ca3",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7920c1c4-6133-4ae8-b3c5-7432b769060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (BATCH_SIZE, 3, 32, 32)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(batch[0].shape)\n",
    "test_idx = 2\n",
    "plt.imshow(batch[0][test_idx].permute(1, 2, 0))\n",
    "plt.title(f\"{fine_labels[batch[1][test_idx]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c563de37-02ca-4cdb-beba-af0cbf6c095f",
   "metadata": {},
   "source": [
    "# Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da45ed94-79f3-4a2f-a703-688ad288c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, device):\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "        for step, (img, label) in enumerate(train_loader):\n",
    "\n",
    "            img, label = img.to(device), label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            train_losses.append(loss.item())  # every step\n",
    "            loss.backward()\n",
    "\n",
    "            # Monitoring overall gradient norm\n",
    "            grads = [\n",
    "                param.grad.detach().flatten()\n",
    "                for param in model.parameters()\n",
    "                if param.grad is not None\n",
    "            ]\n",
    "            norm = torch.cat(grads).norm()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if step % PRINT_ITERS == 0 and step != 0:\n",
    "                val_loss, val_acc = eval(model, val_loader, criterion, device)\n",
    "                val_losses.append(val_loss)\n",
    "                val_accuracies.append(val_acc)\n",
    "                print(\n",
    "                    f\"Step: {step}/{len(train_loader)}, Running Average Loss: {np.mean(train_losses):.3f} |\",\n",
    "                    f\"Val Loss: {val_loss:.3f} | Val Acc: {val_acc:.3f} | Grad Norm: {norm:.2f}\",\n",
    "                )\n",
    "                model.train()\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            },\n",
    "            f\"{path}/checkpoints/{MODEL_NAME}_EPOCH_{epoch+1}_SEED_{SEED}.pt\",\n",
    "        )\n",
    "\n",
    "        with open(\n",
    "            f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_train_losses.json\", \"w\"\n",
    "        ) as f:\n",
    "            json.dump(train_losses, f)\n",
    "\n",
    "        with open(\n",
    "            f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_val_losses.json\", \"w\"\n",
    "        ) as f2:\n",
    "            json.dump(val_losses, f2)\n",
    "\n",
    "        with open(\n",
    "            f\"{path}/train_logs/{MODEL_NAME}_SEED_{SEED}_val_accuracies.json\", \"w\"\n",
    "        ) as f3:\n",
    "            json.dump(val_accuracies, f3)\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7c26eef-743f-48a1-ba71-f11e04cd4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, val_loader, criterion, device):\n",
    "    val_losses = []\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (img, label) in enumerate(val_loader):\n",
    "\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            out = model(img)\n",
    "\n",
    "            loss_eval = criterion(out, label)\n",
    "            val_losses.append(loss_eval.item())\n",
    "\n",
    "            pred = out.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_acc = correct / ((len(val_loader) - 1) * BATCH_SIZE + label.size(0))\n",
    "\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa1e076c-a6b4-42b3-9401-b36769cc2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer = get_vit_and_optimizer(seed=SEED)\n",
    "model.to(device)\n",
    "# summary(model)\n",
    "\n",
    "# since no log-softmax output layer in model\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07e165-3754-4814-b7e4-86a956164682",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Driver code\n",
    "train_losses, val_losses, val_accuracies = train(\n",
    "    model, train_loader, val_loader, optimizer, criterion, device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
