{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4b5f20-1af9-4c76-9817-13a502b84e84",
   "metadata": {},
   "source": [
    "# Gated Knowledge Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb95043-53a1-4690-aaee-69299f3f93cf",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2201.05629"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8b9b82-cd89-4401-aa1d-a2ee01675e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209e2728-6ead-40f7-96f0-1f2ef7646a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = None\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7be4ef1-65d0-4d0a-8b3c-e54154f2f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a5a33d-7779-458d-a0bd-678e15f207e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "path = path if drive is None else \"/content/drive/MyDrive/self-learn/unlearning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbbd3f0a-951f-442b-a838-82d0177bb616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from constants import *\n",
    "from utils import set_seed, train_data, val_data, \\\n",
    "                    train_loader, val_loader, fine_labels\n",
    "from models import get_model_and_optimizer\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f295fa22-3134-4c48-a717-106da164bb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: CNN_CIFAR_100_ORIGINAL\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = (\n",
    "    f\"CNN_CIFAR_100_ORIGINAL\"\n",
    ")\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbaed7e-f154-4f09-9dc7-23d984921c83",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764328cc-4458-4879-9c19-88caad82698a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cloud'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_class = 23\n",
    "fine_labels[target_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e85115-1540-438a-b44d-f02d47a979a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, val_loader, criterion, device):\n",
    "    val_losses = []\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (img, label) in enumerate(val_loader):\n",
    "          \n",
    "            img, label = img.to(device), label.to(device)\n",
    "            out = model(img)\n",
    "            \n",
    "            loss_eval = criterion(out, label)\n",
    "            val_losses.append(loss_eval.item())\n",
    "            \n",
    "            pred = out.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_acc = correct / (len(val_loader) * BATCH_SIZE)\n",
    "    \n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a105a27-a151-42ca-83b7-4b681167763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_idx = np.where(np.array(train_data.targets) == target_class)[0]\n",
    "forget_mask = np.zeros(len(train_data.targets), dtype=bool)\n",
    "forget_mask[forget_idx] = True\n",
    "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
    "\n",
    "forget_data = torch.utils.data.Subset(train_data, forget_idx)\n",
    "retain_data = torch.utils.data.Subset(train_data, retain_idx)\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(forget_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "retain_loader = torch.utils.data.DataLoader(retain_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d9f59-f463-4125-bb03-c5c667583cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_EPOCH = 100\n",
    "\n",
    "t_model, _ = get_model_and_optimizer(return_act=True)\n",
    "t_model.load_state_dict(torch.load(f\"{path}/checkpoints/{MODEL_NAME}_EPOCH_{LOAD_EPOCH}_SEED_{SEED}.pt\",\n",
    "                                  map_location=device)[\"model_state_dict\"])\n",
    "t_model.to(device)\n",
    "print('Teacher Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f43aa316-ec64-4c07-9546-0164fb9c1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f60d8-b657-426d-a1e3-9af4a102be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize student as random\n",
    "st_model, st_optimizer = get_model_and_optimizer(return_act=True)\n",
    "st_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425594db-cc47-4227-8a88-8b19894d151b",
   "metadata": {},
   "source": [
    "# IMMEDIATE IMMEDIATE TODO: test out instantianting model w/ return_act=False and with True, see that it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6298594e-ee2d-468e-b6b2-d151dace6f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TOTOOSODOTOTODTODOTODTODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bf986-ce68-4ba8-a4a7-ee32546d2cc3",
   "metadata": {},
   "source": [
    "# TODO: GKT Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3e3283-c099-49fd-a6a5-e4ec6561eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load trained model as teacher, student as random, define generator class and instantiate as random also\n",
    "\n",
    "## for n epochs, create random noise z:\n",
    "    ## inner loop to train generator:\n",
    "        ## pass z to generator to create pseudo sample x. Pass through band-pass filter, \n",
    "        ## then compute negative KL divergence of T(x) || S(x) to maximize distance. End\n",
    "    ## enter second loop to train student—sample from generator, apply filter, \n",
    "    ## use student loss function (KL divergence + β (HP) * L_{at}).\n",
    "## end inner and outer loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f195ee-791d-4aab-b8f8-19c5099cf8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## copied directly from repo\n",
    "def attention(x):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/szagoruyko/attention-transfer\n",
    "    :param x = activations\n",
    "    \"\"\"\n",
    "    return F.normalize(x.pow(2).mean(1).view(x.size(0), -1))\n",
    "\n",
    "def attention_diff(x, y):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/szagoruyko/attention-transfer\n",
    "    :param x = activations\n",
    "    :param y = activations\n",
    "    \"\"\"\n",
    "    return (attention(x) - attention(y)).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e15ea3-a7c1-4e0e-a1c5-52a2b3cf88dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(st_logits, t_logits):\n",
    "\n",
    "    t_probs = F.softmax(t_logits, dim=-1)\n",
    "    st_log_probs = F.log_softmax(st_logits, dim=-1) # F.kl_div expects log softmax in first arg\n",
    "    return -F.kl_div(st_log_probs, t_probs, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b8a0c-95f9-4c15-bfa8-3e07485d8324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gkt_loss(st_logits, t_logits, st_act, t_act):\n",
    "\n",
    "    t_probs = F.softmax(t_logits, dim=-1)\n",
    "    st_log_probs = F.log_softmax(st_logits, dim=-1) # F.kl_div expects log softmax in first arg\n",
    "    kl_div = F.kl_div(st_log_probs, t_probs, reduction='mean')\n",
    "\n",
    "    attn_loss = 0\n",
    "    for i in range(len(student_activations)):\n",
    "        attn_loss += ATTN_BETA * attention_diff(st_act[i], t_act[i])\n",
    "\n",
    "    return kl_div + attn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07531afe-e208-42e3-8e35-c2679f51032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(Z_DIM, 512, kernel_size=6, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f6dc97-970f-4ab8-a99e-a2b9a62a8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize generator\n",
    "set_seed()\n",
    "gen = Generator().to(device)\n",
    "gen_optimizer = torch.optim.AdamW(gen.parameters(), lr=GEN_LR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0bb59-42bb-4b46-9b61-0828db5a6fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(gen, st_model, t_model, device):\n",
    "\n",
    "    gen_optimizer.zero_grad()\n",
    "    # z = torch.randn(BATCH_SIZE, Z_DIM).to(device)\n",
    "    # img = gen(z)\n",
    "    ## TODO: apply band pass filter via BAND_PASS_THRESH, yield x_pseudo\n",
    "\n",
    "    st_logits, st_act = st_model(x_pseudo)\n",
    "    t_logits, t_act = t_model(x_pseudo)\n",
    "    ### note if we separate into a new fn then act is not actually needed for this part, training generator right?\n",
    "    loss = generator_loss(st_logits, t_logits)\n",
    "    loss.backward()\n",
    "    ## todo: append loss and stuff\n",
    "    gen_optimizer.step()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff24980-bf17-4a9a-882b-6d17763afda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gkt_unlearn(st_model, t_model, device):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def29fd-a3cb-48d5-adf9-70d3d3069d7c",
   "metadata": {},
   "source": [
    "# Driver code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda1fa0e-4195-4bc2-bf6c-f81f5e6eef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c500f2-98db-4407-8e55-e82782ae5788",
   "metadata": {},
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "51e852dd-9b41-4358-b2ad-fcf6307693ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use shuffle for more interesting results\n",
    "val_viz_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "forget_viz_loader = torch.utils.data.DataLoader(forget_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798b0f4-55e1-47b0-967d-50331fd95616",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # choose one batch from val and one batch from forget\n",
    "    for (val_img, val_label), (forget_img, forget_label) in zip(val_viz_loader, forget_viz_loader):\n",
    "        viz_img, viz_label = torch.cat([val_img, forget_img]), torch.cat([val_label, forget_label])\n",
    "        viz_img, viz_label = viz_img.to(device), viz_label.to(device)\n",
    "        out = model(viz_img)\n",
    "        pred = out.argmax(dim=-1)\n",
    "        break\n",
    "\n",
    "# assumes BATCH_SIZE=8\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16,12))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.set_title(f\"Pred: {fine_labels[pred[i]]} | Label: {fine_labels[viz_label[i]]}\", fontsize=8)\n",
    "    ax.imshow(invTrans(viz_img[i]).cpu().permute(1,2,0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
