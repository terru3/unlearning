{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4b5f20-1af9-4c76-9817-13a502b84e84",
   "metadata": {},
   "source": [
    "# Unlearning by Selective Impair and Repair (UNSIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb95043-53a1-4690-aaee-69299f3f93cf",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/2111.08947"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8b9b82-cd89-4401-aa1d-a2ee01675e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209e2728-6ead-40f7-96f0-1f2ef7646a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive = None\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7be4ef1-65d0-4d0a-8b3c-e54154f2f7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./\"\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38a5a33d-7779-458d-a0bd-678e15f207e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "path = path if drive is None else \"/content/drive/MyDrive/self-learn/unlearning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbbd3f0a-951f-442b-a838-82d0177bb616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from constants import *\n",
    "from utils import set_seed, train_data, val_data, \\\n",
    "                    train_loader, val_loader, fine_labels\n",
    "from models import get_model_and_optimizer\n",
    "    \n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f295fa22-3134-4c48-a717-106da164bb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name: CNN_CIFAR_100_ORIGINAL\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = (\n",
    "    f\"CNN_CIFAR_100_ORIGINAL\"\n",
    ")\n",
    "print(\"Model Name:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbaed7e-f154-4f09-9dc7-23d984921c83",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764328cc-4458-4879-9c19-88caad82698a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cloud'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_class = 23\n",
    "fine_labels[target_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e85115-1540-438a-b44d-f02d47a979a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, val_loader, criterion, device):\n",
    "    val_losses = []\n",
    "    correct = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (img, label) in enumerate(val_loader):\n",
    "          \n",
    "            img, label = img.to(device), label.to(device)\n",
    "            out = model(img)\n",
    "            \n",
    "            loss_eval = criterion(out, label)\n",
    "            val_losses.append(loss_eval.item())\n",
    "            \n",
    "            pred = out.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_acc = correct / (len(val_loader) * BATCH_SIZE)\n",
    "    \n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a105a27-a151-42ca-83b7-4b681167763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_idx = np.where(np.array(train_data.targets) == target_class)[0]\n",
    "forget_mask = np.zeros(len(train_data.targets), dtype=bool)\n",
    "forget_mask[forget_idx] = True\n",
    "retain_idx = np.arange(forget_mask.size)[~forget_mask]\n",
    "\n",
    "forget_data = torch.utils.data.Subset(train_data, forget_idx)\n",
    "retain_data = torch.utils.data.Subset(train_data, retain_idx)\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(forget_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "retain_loader = torch.utils.data.DataLoader(retain_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1181bd9-139e-4c31-8fc1-2abf3cab0162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and optimizer loaded\n"
     ]
    }
   ],
   "source": [
    "LOAD_EPOCH = 100\n",
    "\n",
    "model, optimizer = get_model_and_optimizer()\n",
    "model.load_state_dict(torch.load(f\"{path}/checkpoints/{MODEL_NAME}_EPOCH_{LOAD_EPOCH}_SEED_{SEED}.pt\",\n",
    "                                  map_location=device)[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(torch.load(f\"{path}/checkpoints/{MODEL_NAME}_EPOCH_{LOAD_EPOCH}_SEED_{SEED}.pt\",\n",
    "                                  map_location=device)[\"optimizer_state_dict\"])\n",
    "model.to(device)\n",
    "print('Model and optimizer loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78cf160b-5502-4503-a58c-dec9b9b9193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model = copy.deepcopy(model) # archive trained model for logging/comparison purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f43aa316-ec64-4c07-9546-0164fb9c1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3bf986-ce68-4ba8-a4a7-ee32546d2cc3",
   "metadata": {},
   "source": [
    "# UNSIR Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a2c067b-7686-483a-a231-309b77293dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(nn.Module):\n",
    "    def __init__(self, *dim):\n",
    "        super().__init__()\n",
    "        self.noise = torch.nn.Parameter(torch.randn(*dim), requires_grad = True)\n",
    "        \n",
    "    def forward(self):\n",
    "        return self.noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf62a1f9-133a-4b02-9515-1e880a554009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_noise(model, noises, noise_optimizer, target_class, device):\n",
    "    noise_train_losses = []\n",
    "    for i in range(NOISE_STEPS):\n",
    "        noise_optimizer.zero_grad()\n",
    "        input = noises[target_class]().to(device)\n",
    "        label = torch.full((BATCH_SIZE,), target_class)\n",
    "        out = model(input)\n",
    "        loss = -criterion(out, label) + NOISE_LAMBDA*torch.mean(torch.sum(torch.square(input), [1, 2, 3]))\n",
    "        loss.backward()\n",
    "        noise_train_losses.append(loss.item())\n",
    "        noise_optimizer.step()\n",
    "            \n",
    "        if i % 20 == 0:\n",
    "            print(f\"Step: {i}/{NOISE_STEPS} | Running Noise Loss: {np.mean(noise_train_losses):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faf9633e-e2b5-44fb-8035-1c90f46dc7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impair(model, impair_optimizer, noise_loader, device):\n",
    "    model.train()\n",
    "    impair_train_losses = []\n",
    "    for epoch in range(IMPAIR_EPOCHS):\n",
    "        for step, (img, label) in enumerate(noise_loader):\n",
    "            img, label = img.to(device), label.to(device)\n",
    "        \n",
    "            impair_optimizer.zero_grad()\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            loss.backward()\n",
    "            impair_train_losses.append(loss.item())\n",
    "            impair_optimizer.step()\n",
    "\n",
    "            if step % 100 == 0:\n",
    "                print(f\"Step: {step}/{len(noise_loader)}, Running Impair Loss: {np.mean(impair_train_losses):.3f}\")\n",
    "            \n",
    "        # torch.save(\n",
    "        #     {\n",
    "        #         \"model_state_dict\": model.state_dict(),\n",
    "        #         \"optimizer_state_dict\": impair_optimizer.state_dict(),\n",
    "        #     },\n",
    "        #     f\"{path}/checkpoints/{MODEL_NAME}_IMPAIR_EPOCH_{epoch+1}_SEED_{SEED}.pt\",\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53083879-93ff-4fe6-bf19-4a79f9f20ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repair(model, repair_optimizer, retain_loader, device):\n",
    "    model.train()\n",
    "    repair_train_losses = []\n",
    "    for epoch in range(REPAIR_EPOCHS):\n",
    "        for step, (img, label) in enumerate(retain_loader):\n",
    "            img, label = img.to(device), label.to(device)\n",
    "        \n",
    "            repair_optimizer.zero_grad()\n",
    "            out = model(img)\n",
    "            loss = criterion(out, label)\n",
    "            loss.backward()\n",
    "            repair_train_losses.append(loss.item())\n",
    "            repair_optimizer.step()\n",
    "\n",
    "            if step % 150 == 0:\n",
    "                print(f\"Step: {step}/{len(retain_loader)}, Running Repair Loss: {np.mean(repair_train_losses):.3f}\")\n",
    "            \n",
    "        # torch.save(\n",
    "        #     {\n",
    "        #         \"model_state_dict\": model.state_dict(),\n",
    "        #         \"optimizer_state_dict\": repair_optimizer.state_dict(),\n",
    "        #     },\n",
    "        #     f\"{path}/checkpoints/{MODEL_NAME}_REPAIR_EPOCH_{epoch+1}_SEED_{SEED}.pt\",\n",
    "        # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def29fd-a3cb-48d5-adf9-70d3d3069d7c",
   "metadata": {},
   "source": [
    "# Driver code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e16f13-e9bc-4935-b971-ec30c2ad43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### prior to unlearning\n",
    "\n",
    "# forget and val data accuracy\n",
    "eval(model, forget_loader, criterion, device)[1], eval(model, val_loader, criterion, device)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05d35b5-d43d-4512-a6fb-bf2509e9b9de",
   "metadata": {},
   "source": [
    "### Step 1: Train noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72fa96fb-358b-4af0-b3b2-80128d528be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "noises = {}\n",
    "set_seed()\n",
    "noises[target_class] = Noise(BATCH_SIZE, 3, 32, 32).to(device)\n",
    "noise_optimizer = torch.optim.AdamW(noises[target_class].parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6b828c2-eaed-453c-9068-7796cd9ef53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0/250 | Running Noise Loss: 300.155\n",
      "Step: 20/250 | Running Noise Loss: 93.434\n",
      "Step: 40/250 | Running Noise Loss: 42.197\n",
      "Step: 60/250 | Running Noise Loss: 22.975\n",
      "Step: 80/250 | Running Noise Loss: 11.528\n",
      "Step: 100/250 | Running Noise Loss: 3.436\n",
      "Step: 120/250 | Running Noise Loss: -0.581\n",
      "Step: 140/250 | Running Noise Loss: -2.425\n",
      "Step: 160/250 | Running Noise Loss: -1.798\n",
      "Step: 180/250 | Running Noise Loss: -2.443\n",
      "Step: 200/250 | Running Noise Loss: -2.802\n",
      "Step: 220/250 | Running Noise Loss: -2.944\n",
      "Step: 240/250 | Running Noise Loss: -3.520\n"
     ]
    }
   ],
   "source": [
    "train_noise(model, noises, noise_optimizer, target_class, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2015e6-d0c1-4d88-adca-ebe593f29f18",
   "metadata": {},
   "source": [
    "### Step 2: Impair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc291c20-864f-47dc-a419-1b59878b77bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prep noisy data loader, combine with retain data\n",
    "noise_data = []\n",
    "\n",
    "for i in range(NUM_NOISE_BATCHES):\n",
    "    batch = noises[target_class]().cpu().detach()\n",
    "    for i in range(BATCH_SIZE):\n",
    "        noise_data.append((batch[i], target_class))\n",
    "\n",
    "## Instead of adding all of retain_data, add only ~14% of it\n",
    "# maybe just temporary?\n",
    "subset_idx = list(range(0, len(retain_data), 7))\n",
    "retain_data_subset = torch.utils.data.Subset(retain_data, subset_idx)\n",
    "noise_data += retain_data_subset\n",
    "\n",
    "# noise_data += retain_data # takes 15 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "797eafb5-8330-4f40-bb60-4c0032fedf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_loader = torch.utils.data.DataLoader(noise_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "impair_optimizer = torch.optim.AdamW(model.parameters(), lr=IMPAIR_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ac6eeff-c514-44f1-a4e7-b403a88e934f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0/934, Running Impair Loss: 4.079\n",
      "Step: 100/934, Running Impair Loss: 5.034\n",
      "Step: 200/934, Running Impair Loss: 4.850\n",
      "Step: 300/934, Running Impair Loss: 4.763\n",
      "Step: 400/934, Running Impair Loss: 4.724\n",
      "Step: 500/934, Running Impair Loss: 4.688\n",
      "Step: 600/934, Running Impair Loss: 4.672\n",
      "Step: 700/934, Running Impair Loss: 4.658\n",
      "Step: 800/934, Running Impair Loss: 4.648\n",
      "Step: 900/934, Running Impair Loss: 4.639\n"
     ]
    }
   ],
   "source": [
    "impair(model, impair_optimizer, noise_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eb071e9-07bd-4048-9e96-6d94664fb4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9821428571428571, 0.01)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forget and val data accuracy\n",
    "eval(model, forget_loader, criterion, device)[1], eval(model, val_loader, criterion, device)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "710ad8ed-ca36-4e51-8aef-293b860aeecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([23, 23, 23, 23, 23, 23, 23, 23])\n",
      "tensor([23, 23, 23, 23, 23, 23, 23, 23])\n",
      "tensor([23, 23, 23, 23, 23, 23, 23, 23])\n",
      "tensor([23, 23, 23, 23, 23, 23, 23, 23])\n",
      "tensor([23, 23, 23, 23, 23, 23, 23, 23])\n",
      "tensor([23, 23, 23, 23, 23, 23, 23, 23])\n",
      "tensor([23, 23, 23, 23, 23, 23, 23, 23])\n",
      "tensor([23, 23, 23, 23, 23, 23, 23, 23])\n",
      "tensor([23, 23, 23, 23, 23, 23, 23, 23])\n",
      "tensor([23, 23, 23, 23, 23, 23, 23, 61])\n",
      "tensor([23, 23, 23, 23, 23, 23, 23, 23])\n"
     ]
    }
   ],
   "source": [
    "for i, (img, label) in enumerate(val_loader):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(model(img).argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81933c9-2139-448d-86b1-9cc4c3a1c268",
   "metadata": {},
   "source": [
    "## todo: fix!! model gets trained to predict the forget class on everything.....99% forget, 1% val\n",
    "\n",
    "### debugging notes:\n",
    "### training on just forget gives similar 99% result but that's expected\n",
    "### training on noise data without first training the noise itself (skipping step 1) leads to same problem immediately after 100 steps...so it's not step 1 that's the problem\n",
    "### sometimes it makes the model predict a different class, but also constantly that class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13441de-bfa0-426d-90ff-6b567d0e2af0",
   "metadata": {},
   "source": [
    "### Step 3: Repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab2c6ea2-b32e-44cc-b65f-bba8481985c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repair_optimizer = torch.optim.AdamW(model.parameters(), lr=REPAIR_LR)\n",
    "retain_subset_loader = torch.utils.data.DataLoader(retain_data_subset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9b40b97-ddb3-4c4b-a994-023b0664f780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0/884, Running Repair Loss: 4.654\n",
      "Step: 150/884, Running Repair Loss: 4.645\n",
      "Step: 300/884, Running Repair Loss: 4.633\n",
      "Step: 450/884, Running Repair Loss: 4.626\n",
      "Step: 600/884, Running Repair Loss: 4.622\n",
      "Step: 750/884, Running Repair Loss: 4.620\n"
     ]
    }
   ],
   "source": [
    "repair(model, repair_optimizer, retain_subset_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f05ffc6a-64db-40e3-8d81-8315d872b5fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.01)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### after unlearning\n",
    "\n",
    "# forget and val data accuracy\n",
    "eval(model, forget_loader, criterion, device)[1], eval(model, val_loader, criterion, device)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56056ad0-6c46-4ddf-8113-fac6d9fa721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([82, 82, 82, 82, 82, 82, 82, 82])\n",
      "tensor([82, 82, 82, 82, 82, 82, 82, 82])\n",
      "tensor([82, 82, 82, 82, 82, 82, 82, 82])\n",
      "tensor([82, 82, 82, 82, 82, 82, 82, 82])\n",
      "tensor([82, 82, 82, 82, 82, 82, 82, 82])\n",
      "tensor([82, 82, 82, 82, 82, 81, 82, 82])\n",
      "tensor([82, 82, 82, 82, 82, 82, 82, 82])\n",
      "tensor([82, 82, 82, 82, 82, 82, 82, 82])\n",
      "tensor([82, 82, 82, 82, 82, 82, 82, 82])\n",
      "tensor([82, 82, 82, 82, 82, 82, 82, 82])\n",
      "tensor([82, 82, 82, 82, 82, 82, 82, 82])\n"
     ]
    }
   ],
   "source": [
    "for i, (img, label) in enumerate(val_loader):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(model(img).argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea1caf-f03d-4e8b-ba2b-8d6dc71cca53",
   "metadata": {},
   "source": [
    "## todo: fix. repair destroys forget perf without repairing val. now it keeps predicting a diff class\n",
    "#### or sometimes when you don't train enough it does nothing at all and you need to train more to get above result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c500f2-98db-4407-8e55-e82782ae5788",
   "metadata": {},
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "51e852dd-9b41-4358-b2ad-fcf6307693ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use shuffle for more interesting results\n",
    "val_viz_loader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "forget_viz_loader = torch.utils.data.DataLoader(forget_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f6734-a675-4e3a-b174-600cb712b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # choose one batch from val and one batch from forget\n",
    "    for (val_img, val_label), (forget_img, forget_label) in zip(val_viz_loader, forget_viz_loader):\n",
    "        viz_img, viz_label = torch.cat([val_img, forget_img]), torch.cat([val_label, forget_label])\n",
    "        viz_img, viz_label = viz_img.to(device), viz_label.to(device)\n",
    "        out = model(viz_img)\n",
    "        pred = out.argmax(dim=-1)\n",
    "        break\n",
    "\n",
    "# assumes BATCH_SIZE=8\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16,12))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    ax.set_title(f\"Pred: {fine_labels[pred[i]]} | Label: {fine_labels[viz_label[i]]}\", fontsize=8)\n",
    "    ax.imshow(invTrans(viz_img[i]).cpu().permute(1,2,0))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
